% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LLM_calls.R
\name{interrogate_llm}
\alias{interrogate_llm}
\title{Interrogate Language Model}
\usage{
interrogate_llm(
  messages = NULL,
  provider = c("openai", "azure", "local"),
  params = list(temperature = 0),
  force_json = FALSE,
  ...
)
}
\arguments{
\item{messages}{Messages to be sent to the language model.}

\item{provider}{The provider of the language model. Defaults to "openai".
Other options are "azure" and "local".}

\item{params}{Additional parameters for the language model request. Defaults
to a list with \code{temperature = 0}.}

\item{force_json}{A boolean to force the response in JSON format. Default is
FALSE. Works only for OpenAI and Azure endpoints.}

\item{...}{Additional arguments passed to the language model provider
functions.}
}
\value{
Returns the content of the message from the language model response.
}
\description{
This function sends requests to a specified language model provider (OpenAI,
Azure, or a locally running LLM server) and returns the response. It handles
rate limiting and retries the request if necessary, and also processes errors
in the response.
}
\examples{
\dontrun{
response <- interrogate_llm(
 messages = c(user = "Hello there!"),
 provider = "openai")
 }

}
