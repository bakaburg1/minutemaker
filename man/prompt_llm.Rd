% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LLM_calls.R
\name{prompt_llm}
\alias{prompt_llm}
\title{Interrogate a Language Model}
\usage{
prompt_llm(
  messages = NULL,
  provider = getOption("minutemaker_llm_provider"),
  params = list(temperature = 0),
  force_json = FALSE,
  log_request = getOption("minutemaker_log_requests", TRUE),
  ...
)
}
\arguments{
\item{messages}{Messages to be sent to the language model.}

\item{provider}{The provider of the language model. Defaults to "openai".
Other options are "azure" and "local".}

\item{params}{Additional parameters for the language model request. Defaults
to a list with \code{temperature = 0}.}

\item{force_json}{A boolean to force the response in JSON format. Default is
FALSE. Works only for OpenAI and Azure endpoints.}

\item{log_request}{A boolean to log the request time. Can be set up globally
using the \code{minutemaker_log_requests} option, which defaults to TRUE.}

\item{...}{Additional arguments passed to the language model provider
functions.}
}
\value{
Returns the content of the message from the language model response.
}
\description{
This function sends requests to a specified language model provider (OpenAI,
Azure, or a locally running LLM server) and returns the response. It handles
rate limiting and retries the request if necessary, and also processes errors
in the response.
}
\details{
Users can provide their own models by writing a function with the following
name pattern: \verb{use_<model_name>_llm}. See the existing functions using the
::: operator for examples.
}
\examples{
\dontrun{
response <- prompt_llm(
 messages = c(user = "Hello there!"),
 provider = "openai")
 }

}
