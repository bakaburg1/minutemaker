% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LLM_calls.R
\name{use_local_llm}
\alias{use_local_llm}
\title{Use Local Language Model}
\usage{
use_local_llm(
  body,
  endpoint = getOption("minutemaker_local_endpoint_gpt",
    "http://localhost:1234/v1/chat/completions")
)
}
\arguments{
\item{body}{The body of the request.}

\item{endpoint}{The local endpoint for the language model service. Can be
obtained from R options.}
}
\value{
The function returns the response from the local language model
endpoint.
}
\description{
Sends a request to a local language model endpoint using the parameters in
the \code{body} argument. The endpoint URL should be set in the R options, with a
default provided.
}
