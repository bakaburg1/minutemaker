# Generated by Gemini
# tests/testthat/test-transcript_importation.R

# Helper functions ----

# Helper to create a temporary JSON file for testing parse_transcript_json
helper_create_temp_json <- function(
  content_list,
  dir,
  file_name_prefix = "transcript"
) {
  file_paths <- character(length(content_list))
  for (i in seq_along(content_list)) {
    current_prefix <- if (
      length(file_name_prefix) > 1 &&
      length(file_name_prefix) == length(content_list)) {
      file_name_prefix[i]
    } else {
      file_name_prefix
    }
    file_path <- file.path(dir, paste0(current_prefix, "_", i, ".json"))
    jsonlite::write_json(content_list[[i]], file_path, auto_unbox = TRUE)
    file_paths[i] <- file_path
  }
  if (length(file_paths) == 1) {
    return(file_paths[1])
  }
  return(dir)
}

# Helper to create a temporary SRT file
helper_create_temp_srt <- function(
  lines,
  dir,
  file_name = "subtitle.srt"
) {
  file_path <- file.path(dir, file_name)
  writeLines(text = lines, con = file_path)
  return(file_path)
}

# Helper to create a temporary VTT file
helper_create_temp_vtt <- function(
  lines,
  dir,
  file_name = "subtitle.vtt"
) {
  file_path <- file.path(dir, file_name)
  writeLines(text = lines, con = file_path)
  return(file_path)
}

# Helper to create a temporary chat file
helper_create_temp_chat_file <- function(
  lines,
  dir,
  file_name = "chat.txt"
) {
  file_path <- file.path(dir, file_name)
  if (length(lines) == 0) {
    formatted_lines <- character(0)
  } else {
    formatted_lines <- sapply(lines, function(line) {
      if (is.character(line) && !grepl("\\t", line)) {
        return(paste(line, collapse = "\\t"))
      }
      return(line)
    })
  }
  writeLines(text = formatted_lines, con = file_path, useBytes = FALSE)
  return(file_path)
}

# Tests for parse_transcript_json() ----

test_that("parses a single valid JSON file correctly", {
  temp_dir_for_test <- withr::local_tempdir()
  json_content <- list(
    segments = list(
      list(start = 0, end = 1, text = "Hello unique alpha"),
      list(start = 1, end = 2, text = "World unique beta")
    )
  )
  temp_json_file <- helper_create_temp_json(
    list(json_content),
    dir = temp_dir_for_test
  )
  normalized_path <- normalizePath(temp_json_file, mustWork = TRUE)

  result <- parse_transcript_json(normalized_path, pretty_times = FALSE)

  expected <- dplyr::tibble(
    doc = c(1L, 1L),
    file = rep(basename(temp_json_file), 2),
    start = c(0, 1),
    end = c(1, 2),
    text = c("Hello unique alpha", "World unique beta"),
    speaker = rep(NA_character_, 2)
  )

  expect_equal(result, expected)
})

test_that("parses a directory of JSON files correctly and orders them", {
  temp_dir_for_test <- withr::local_tempdir()
  json_content1 <- list(
    segments = list(
      list(start = 0, end = 1, text = "First file segment unique"))
  )

  json_content2 <- list(
    segments = list(
      list(start = 0, end = 2, text = "Second file segment unique"))
  )
  json_content10 <- list(
    segments = list(
      list(start = 0, end = 3, text = "Tenth file segment unique"))
  )

  created_dir_path_from_helper <- helper_create_temp_json(
    list(json_content10, json_content1, json_content2),
    dir = temp_dir_for_test,
    file_name_prefix = c("segment_10", "segment_1", "segment_2")
  )
  normalized_dir_path <- created_dir_path_from_helper |>
    normalizePath(mustWork = TRUE)

  result <- parse_transcript_json(normalized_dir_path, pretty_times = FALSE)

  expected <- dplyr::tibble(
    doc = c(1L, 2L, 3L),
    file = c("segment_1_2.json", "segment_2_3.json", "segment_10_1.json"),
    start = c(0, 1, 3),
    end = c(1, 3, 6),
    text = c(
      "First file segment unique",
      "Second file segment unique",
      "Tenth file segment unique"
    ),
    speaker = rep(NA_character_, 3)
  )

  expect_equal(result, expected)
})

test_that("parses a JSON string correctly", {
  json_string <- '{
    "segments": [
      {
        "start": 0,
        "end": 1,
        "text": "From string unique gamma",
        "speaker": "SpeakerA"
      }
    ]
  }'

  # Expected result: Use tibble for consistency with parse_transcript_json output
  expected <- dplyr::tibble(
    doc = 1L,
    file = NA_character_,
    start = 0,
    end = 1,
    text = "From string unique gamma",
    speaker = "SpeakerA"
  )

  result <- parse_transcript_json(json_string, pretty_times = FALSE)
  expect_equal(result, expected)
})

test_that("parses a list object correctly", {
  json_list <- list(
    segments = list(
      list(start = 0, end = 1, text = "From list unique delta")
    )
  )

  # Expected result
  expected <- dplyr::tibble(
    doc = 1L,
    file = NA_character_,
    start = 0,
    end = 1,
    text = "From list unique delta",
    speaker = NA_character_
  )

  result <- parse_transcript_json(json_list, pretty_times = FALSE)
  expect_equal(result, expected)

})

test_that("adds pretty times when pretty_times is TRUE", {
  temp_dir_for_test <- withr::local_tempdir()
  json_content <- list(
    segments = list(
      list(start = 65, end = 125, text = "Test time unique")
    )
  )
  # Use piping for temp_json_file_path creation and rename for clarity
  temp_json_file_path <- helper_create_temp_json(
    list(json_content),
    dir = temp_dir_for_test
  ) |>
    normalizePath(mustWork = TRUE)

  # Call parse_transcript_json once
  result <- parse_transcript_json(temp_json_file_path, pretty_times = TRUE)

  # Define the full expected tibble
  expected <- dplyr::tibble(
    doc = 1L,
    file = basename(temp_json_file_path),
    start = 65,
    end = 125,
    text = "Test time unique",
    speaker = NA_character_, # Speaker column is now always present
    start_formatted = lubridate::seconds_to_period(65),
    end_formatted = lubridate::seconds_to_period(125)
  )

  # Use a single expect_equal for comparison
  expect_equal(result, expected)
})

test_that("adds clock times when event_start_time is provided", {
  temp_dir_for_test <- withr::local_tempdir()
  json_content <- list(
    segments = list(list(start = 10, end = 20, text = "Test clock unique"))
  )
  temp_json_file <- helper_create_temp_json(list(json_content), dir = temp_dir_for_test)
  # Ensure normalized_path is defined once
  normalized_path <- normalizePath(temp_json_file, mustWork = TRUE)

  # Define a base expected tibble for common columns
  base_expected <- dplyr::tibble(
    doc = 1L,
    file = basename(normalized_path),
    start = 10,
    end = 20,
    text = "Test clock unique",
    speaker = NA_character_ # Speaker column is now always present
  )

  # Test with minutemaker_event_start_time option
  withr::with_options(
    list(minutemaker_event_start_time = "10:00:00"),
    {
      result_opt <- parse_transcript_json(normalized_path, pretty_times = FALSE)
      expected_opt <- base_expected |>
        dplyr::mutate(
          start_clock = "10:00:10",
          end_clock = "10:00:20"
        )
      expect_equal(result_opt, expected_opt)
    }
  )

  # Test with event_start_time argument ("09:30 AM")
  result_arg_am_pm <- parse_transcript_json(
    normalized_path,
    pretty_times = FALSE,
    event_start_time = "09:30 AM"
  )
  expected_arg_am_pm <- base_expected |>
    dplyr::mutate(
      start_clock = "09:30:10",
      end_clock = "09:30:20"
    )
  expect_equal(result_arg_am_pm, expected_arg_am_pm)

  # Test with event_start_time argument ("14:15")
  result_arg_hhmm <- parse_transcript_json(
    normalized_path,
    pretty_times = FALSE,
    event_start_time = "14:15"
  )
  expected_arg_hhmm <- base_expected |>
    dplyr::mutate(
      start_clock = "14:15:10",
      end_clock = "14:15:20"
    )
  expect_equal(result_arg_hhmm, expected_arg_hhmm)
})

test_that("handles invalid event_start_time gracefully", {
  temp_dir_for_test <- withr::local_tempdir()
  json_content <- list(segments = list(list(start = 0, end = 1, text = "t unique")))
  temp_json_file <- helper_create_temp_json(list(json_content), dir = temp_dir_for_test)
  normalized_path <- normalizePath(temp_json_file, mustWork = TRUE)

  parse_transcript_json(normalized_path, event_start_time = "Invalid Time") |>
    expect_error("Event start time .* is not interpretable") |>
    expect_warning("All formats failed to parse")

  withr::with_options(
    list(minutemaker_event_start_time = "bad-time"),
    parse_transcript_json(normalized_path) |>
      expect_error("Event start time .* is not interpretable") |>
      expect_warning("All formats failed to parse")
  )
})

test_that("parse_transcript_json uses metadata unless event_start_time is NA", {
  temp_dir <- withr::local_tempdir()
  json_path <- file.path(temp_dir, "segment_1.json")
  jsonlite::write_json(
    list(
      text = "sample",
      segments = list(
        list(start = 0, end = 5, text = "Hello", speaker = "A")
      ),
      transcript_start_time = "09:30 AM"
    ),
    json_path,
    auto_unbox = TRUE,
    pretty = TRUE
  )

  withr::with_options(
    list(minutemaker_event_start_time = NULL),
    {
      parsed_with_metadata <- parse_transcript_json(temp_dir)
      expect_true("start_clock" %in% names(parsed_with_metadata))

      parsed_ignore_metadata <- parse_transcript_json(
        temp_dir,
        event_start_time = NA
      )
      expect_false("start_clock" %in% names(parsed_ignore_metadata))
    }
  )
})

test_that("parse_transcript_json handles date-time metadata start time", {
  dir_path <- testthat::test_path(
    "material",
    "teams_json_out",
    "transcription_output_data"
  )
  skip_if_not(dir.exists(dir_path))

  parsed <- parse_transcript_json(dir_path)
  expect_true(all(c("start_clock", "end_clock") %in% names(parsed)))
})

test_that("handles JSON file missing 'segments' field", {
  temp_dir_for_test_single_file <- withr::local_tempdir()
  json_content_no_segments <- list(some_other_field = "data")
  temp_json_file <- helper_create_temp_json(list(json_content_no_segments), dir = temp_dir_for_test_single_file)
  normalized_path <- normalizePath(temp_json_file, mustWork = TRUE)
  expect_error(
    parse_transcript_json(normalized_path),
    "JSON file is missing expected 'segments' data"
  )
  temp_dir_for_test_multi_file <- withr::local_tempdir()
  helper_create_temp_json(list(json_content_no_segments), dir = temp_dir_for_test_multi_file)
  normalized_dir_path <- normalizePath(temp_dir_for_test_multi_file, mustWork = TRUE)
  expect_error(
    parse_transcript_json(normalized_dir_path),
    "JSON file is missing expected 'segments' data"
  )
})

test_that("handles empty 'segments' list by skipping file with message", {
  temp_dir_for_test_single_file <- withr::local_tempdir()
  json_content_empty_segments <- list(segments = list())
  temp_json_file <- list(json_content_empty_segments) |>
    helper_create_temp_json(dir = temp_dir_for_test_single_file)

  # Expected empty data frame with correct schema
  expected <- dplyr::tibble(
    doc = integer(),
    file = character(),
    start = numeric(),
    end = numeric(),
    text = character(),
    speaker = character(),
    start_formatted = lubridate::period(),
    end_formatted = lubridate::period()
  )

  expect_message(
    result <- normalizePath(temp_json_file, mustWork = TRUE) |>
      parse_transcript_json(),
    "Skipping empty file"
  )
  expect_equal(result, expected)

  temp_dir_for_test_multi_file <- withr::local_tempdir()
  list(json_content_empty_segments) |>
    helper_create_temp_json(dir = temp_dir_for_test_multi_file)
  normalized_dir_path <- temp_dir_for_test_multi_file |>
    normalizePath(mustWork = TRUE)

  expect_message(
    result_dir <- parse_transcript_json(normalized_dir_path),
    "Skipping empty file"
  )
  expect_equal(result_dir, expected)
})

test_that("handles errors when parsing invalid JSON content", {
  temp_dir_for_test_invalid_file <- withr::local_tempdir()
  temp_invalid_json_file <- file.path(temp_dir_for_test_invalid_file, "invalid.json")
  writeLines("This is not JSON { invalid", temp_invalid_json_file)
  normalized_path <- normalizePath(temp_invalid_json_file, mustWork = TRUE)
  expect_error(
    parse_transcript_json(normalized_path),
    "Error parsing JSON file"
  )

  expect_error(
    parse_transcript_json("This is not valid JSON { syntax error"),
    "Error parsing JSON string input"
  )

  temp_dir_with_invalid <- withr::local_tempdir()
  valid_content <- list(segments = list(list(start = 0, end = 1, text = "a unique")))
  helper_create_temp_json(
    list(valid_content), dir = temp_dir_with_invalid, file_name_prefix = "valid_first"
  )
  invalid_file_in_dir <- file.path(temp_dir_with_invalid, "invalid_second.json")
  writeLines("} Not JSON", invalid_file_in_dir)
  normalized_dir_path <- normalizePath(temp_dir_with_invalid, mustWork = TRUE)
  expect_error(
    parse_transcript_json(normalized_dir_path),
    regexp = "Error parsing JSON file\\(s\\) from directory"
  )
})

test_that("handles various data types for optional columns correctly", {
  temp_dir_for_test <- withr::local_tempdir()
  json_content_optional_cols <- list(
    segments = list(
      list(
        start = 0, end = 1, text = "Full data unique",
        speaker = "SpeakerA", avg_logprob = -0.123, no_speech_prob = 0.05
      ),
      list(start = 1, end = 2, text = "Minimal data unique", speaker = "SpeakerB")
    )
  )
  temp_json_file <- helper_create_temp_json(list(json_content_optional_cols), dir = temp_dir_for_test)
  normalized_path <- normalizePath(temp_json_file, mustWork = TRUE)

  result <- parse_transcript_json(normalized_path, pretty_times = FALSE)

  expected <- dplyr::tibble(
    doc = rep(1L, 2), # Use rep for clarity and if rows increase
    file = rep(basename(normalized_path), 2), # Use rep for clarity
    start = c(0, 1),
    end = c(1, 2),
    text = c("Full data unique", "Minimal data unique"),
    speaker = c("SpeakerA", "SpeakerB")
  )
  expect_equal(result, expected)
})

test_that("unlists list columns and handles distinct rows correctly", {
  temp_dir_for_test <- withr::local_tempdir()
  json_content_list_cols <- list(
    segments = list(
      list(start = list(0), end = list(1), text = list("Text in list unique")),
      list(start = list(0), end = list(1), text = list("Text in list unique")),
      list(start = list(1), end = list(2), text = list("Another text unique"))
    )
  )
  temp_json_file <- helper_create_temp_json(list(json_content_list_cols), dir = temp_dir_for_test)
  normalized_path <- normalizePath(temp_json_file, mustWork = TRUE)

  result <- parse_transcript_json(normalized_path, pretty_times = FALSE)

  expected <- dplyr::tibble(
    doc = rep(1L, 2),
    file = rep(basename(normalized_path), 2),
    start = c(0, 1),
    end = c(1, 2),
    text = c("Text in list unique", "Another text unique"),
    speaker = rep(NA_character_, 2) # Speaker column is now always present
  )
  expect_equal(result, expected)
})

test_that("`last_time` accumulates correctly across multiple files including skips", {
  temp_dir_for_test <- withr::local_tempdir()
  json_content1 <- list(segments = list(list(start = 0, end = 10, text = "F1S1 unique")))
  json_content2_empty <- list(segments = list())
  json_content3 <- list(segments = list(list(start = 5, end = 15, text = "F3S1 unique")))

  helper_create_temp_json(list(json_content1), dir = temp_dir_for_test, file_name_prefix = "f_1")
  helper_create_temp_json(list(json_content2_empty), dir = temp_dir_for_test, file_name_prefix = "f_2_empty")
  helper_create_temp_json(list(json_content3), dir = temp_dir_for_test, file_name_prefix = "f_3")

  result <- NULL # Initialize result to be accessible after expect_message
  expect_message(
    result <- parse_transcript_json(normalizePath(temp_dir_for_test, mustWork = TRUE), pretty_times = FALSE),
    "Skipping empty file.*f_2_empty_1.json"
  )

  expected <- dplyr::tibble(
    doc = c(1L, 3L), # doc 2 (f_2_empty) is skipped
    file = c("f_1_1.json", "f_3_1.json"),
    start = c(0, 10 + 5), # start of f3 is relative to its content, then shifted by end of f1
    end = c(10, 10 + 15),  # end of f3 is relative to its content, then shifted by end of f1
    text = c("F1S1 unique", "F3S1 unique"),
    speaker = rep(NA_character_, 2) # Speaker column is now always present
  )
  expect_equal(result, expected)
})

test_that("parse_transcript_json gives specific error for unsupported input type", {
  expect_error(
    parse_transcript_json(12345),
    "Unsupported transcript format"
  )
  expect_error(
    parse_transcript_json(TRUE),
    "Unsupported transcript format"
  )
})

# Tests for import_transcript_from_file() ----

test_that("imports SRT file correctly", {
  temp_dir_for_test <- withr::local_tempdir()
  srt_content <- c(
    "1",
    "00:00:01,000 --> 00:00:02,500",
    "Hello from SRT",
    "",
    "2",
    "00:00:03,100 --> 00:00:04,200",
    "Second line"
  )
  # Use piping for temp_srt_file_path creation and rename
  temp_srt_file_path <- helper_create_temp_srt(srt_content, dir = temp_dir_for_test) |>
    normalizePath(mustWork = TRUE)

  result <- import_transcript_from_file(temp_srt_file_path, import_diarization = FALSE)

  expected <- dplyr::tibble(
    start = c(1.0, 3.1),
    end = c(2.5, 4.2),
    text = c("Hello from SRT", "Second line")
    # No 'speaker' column expected when import_diarization = FALSE
  )
  expect_equal(result, expected)
})

test_that("imports VTT file correctly (standard format, no diarization)", {
  temp_dir_for_test <- withr::local_tempdir()
  vtt_content <- c(
    "WEBVTT",
    "",
    "00:00:01.000 --> 00:00:02.500",
    "Hello from VTT",
    "",
    "NOTE This is a VTT comment and should be ignored",
    "00:00:03.100 --> 00:00:04.200",
    "Second VTT line"
  )
  # Use piping for temp_vtt_file_path creation and rename
  temp_vtt_file_path <- helper_create_temp_vtt(vtt_content, dir = temp_dir_for_test) |>
    normalizePath(mustWork = TRUE)

  result <- import_transcript_from_file(temp_vtt_file_path, import_diarization = FALSE)

  expected <- dplyr::tibble(
    start = c(1.0, 3.1),
    end = c(2.5, 4.2),
    text = c("Hello from VTT", "Second VTT line")
    # No 'speaker' column expected when import_diarization = FALSE
  )
  expect_equal(result, expected)
})

test_that("imports VTT file with MS Teams diarization (<v Speaker>)", {
  temp_dir_for_test <- withr::local_tempdir()
  vtt_content_teams <- c(
    "WEBVTT",
    "",
    "00:00:01.000 --> 00:00:02.500",
    "<v Speaker One>Hello MS Teams format",
    "",
    "00:00:03.100 --> 00:00:04.200",
    "<v Speaker Two Name>Second line here"
  )
  temp_vtt_file <- helper_create_temp_vtt(vtt_content_teams, dir = temp_dir_for_test)

  result_diarized <- import_transcript_from_file(normalizePath(temp_vtt_file, mustWork = TRUE), import_diarization = TRUE)
  expect_s3_class(result_diarized, "data.frame")
  expect_identical(nrow(result_diarized), 2L)
  expect_true("speaker" %in% names(result_diarized))
  expect_identical(result_diarized$speaker, c("Speaker One", "Speaker Two Name"))
  expect_identical(result_diarized$text, c("Hello MS Teams format", "Second line here"))

  result_no_diarization <- import_transcript_from_file(normalizePath(temp_vtt_file, mustWork = TRUE), import_diarization = FALSE)
  expect_s3_class(result_no_diarization, "data.frame")
  expect_false("speaker" %in% names(result_no_diarization))
  expect_identical(result_no_diarization$text, c("Hello MS Teams format", "Second line here"))
})

test_that("imports VTT file with standard cue ID diarization (1 \"Speaker\")", {
  temp_dir_for_test <- withr::local_tempdir()
  vtt_content_cue_id <- c(
    "WEBVTT",
    "",
    "1 \"Speaker Alpha\"",
    "00:00:01.000 --> 00:00:02.500",
    "Alpha says this.",
    "",
    "Cue identifier without speaker",
    "00:00:03.100 --> 00:00:04.200",
    "No speaker info for this line.",
    "",
    "3 \"Speaker Beta Long Name\" some other text after speaker",
    "00:00:05.000 --> 00:00:06.500",
    "Beta says that."
  )
  temp_vtt_file <- helper_create_temp_vtt(vtt_content_cue_id, dir = temp_dir_for_test)
  result <- import_transcript_from_file(normalizePath(temp_vtt_file, mustWork = TRUE), import_diarization = TRUE)

  expect_s3_class(result, "data.frame")
  expect_identical(nrow(result), 3L)
  expect_true("speaker" %in% names(result))
  expect_identical(result$speaker, c("Speaker Alpha", NA_character_, "Speaker Beta Long Name"))
  expect_identical(result$text, c("Alpha says this.", "No speaker info for this line.", "Beta says that."))
})

test_that("handles import_diarization = FALSE correctly for all types", {
  temp_dir_for_test <- withr::local_tempdir()
  srt_content <- c("1", "00:00:01,000 --> 00:00:02,500", "Text that might say Speaker: Foo")
  temp_srt_file <- helper_create_temp_srt(srt_content, dir = temp_dir_for_test)
  result_srt <- import_transcript_from_file(normalizePath(temp_srt_file, mustWork = TRUE), import_diarization = FALSE)
  expect_false("speaker" %in% names(result_srt))
  expect_identical(result_srt$text, "Text that might say Speaker: Foo")

  vtt_content_teams <- c(
    "WEBVTT",
    "",
    "00:00:01.000 --> 00:00:02.500",
    "<v Speaker One>Text content"
  )
  temp_vtt_teams <- helper_create_temp_vtt(vtt_content_teams, dir = temp_dir_for_test, file_name = "vtt_teams.vtt")
  result_vtt_teams <- import_transcript_from_file(normalizePath(temp_vtt_teams, mustWork = TRUE), import_diarization = FALSE)
  expect_false("speaker" %in% names(result_vtt_teams))
  expect_identical(result_vtt_teams$text, "Text content")

  vtt_content_cue <- c(
    "WEBVTT",
    "",
    "1 \"SpeakerX\"",
    "00:00:01.000 --> 00:00:02.500",
    "Cue text"
  )
  temp_vtt_cue <- helper_create_temp_vtt(
    vtt_content_cue,
    dir = temp_dir_for_test,
    file_name = "vtt_cue.vtt"
  )
  result_vtt_cue <- import_transcript_from_file(
    normalizePath(temp_vtt_cue, mustWork = TRUE),
    import_diarization = FALSE
    )
  expect_false("speaker" %in% names(result_vtt_cue))
  expect_identical(result_vtt_cue$text, "Cue text")
})

test_that("handles unsupported file formats with an error", {
  temp_dir_for_test <- withr::local_tempdir()
  temp_txt_file <- file.path(temp_dir_for_test, "temp.txt")
  writeLines("Some text data", temp_txt_file)
  expect_error(
    import_transcript_from_file(normalizePath(temp_txt_file, mustWork = TRUE)),
    "Unsupported transcript format"
  )
  temp_doc_file <- file.path(temp_dir_for_test, "temp.docx")
  writeLines("Word doc", temp_doc_file)
  expect_error(
    import_transcript_from_file(normalizePath(temp_doc_file, mustWork = TRUE)),
    "Unsupported transcript format"
  )
})

test_that("rejects files with no time cues at all", {
  temp_dir_for_test <- withr::local_tempdir()
  srt_no_times <- c("1", "This is not a time cue", "Some text")
  temp_srt_nt <- helper_create_temp_srt(srt_no_times, dir = temp_dir_for_test)
  expect_error(
    import_transcript_from_file(normalizePath(temp_srt_nt, mustWork = TRUE)),
    "does not contain any time cues"
  )

  vtt_no_times <- c("WEBVTT", "", "Only text, no timestamps")
  temp_vtt_nt <- helper_create_temp_vtt(vtt_no_times, dir = temp_dir_for_test)
  expect_error(
    import_transcript_from_file(normalizePath(temp_vtt_nt, mustWork = TRUE)),
    "does not contain any time cues"
  )

  temp_empty_srt <- helper_create_temp_srt(character(0), dir = temp_dir_for_test)
  expect_error(
    import_transcript_from_file(normalizePath(temp_empty_srt, mustWork = TRUE)),
    "does not contain any time cues"
  )

  temp_empty_vtt <- helper_create_temp_vtt(character(0), dir = temp_dir_for_test)
  expect_error(
    import_transcript_from_file(normalizePath(temp_empty_vtt, mustWork = TRUE)),
    "does not contain any time cues"
  )
})

test_that("skips malformed time cues and keeps valid entries", {
  temp_dir_for_test <- withr::local_tempdir()
  vtt_with_bad_cue <- c(
    "WEBVTT",
    "not-a-time --> 00:00:02.000",
    "This line should be skipped",
    "",
    "00:00:05.000 --> 00:00:06.000",
    "Valid line stays"
  )
  temp_vtt_file <- helper_create_temp_vtt(vtt_with_bad_cue, dir = temp_dir_for_test)

  expect_warning(
    result <- import_transcript_from_file(
      normalizePath(temp_vtt_file, mustWork = TRUE),
      import_diarization = FALSE
    ),
    "Skipping malformed time cue"
  )
  expect_identical(nrow(result), 1L)
  expect_identical(result$start, 5)
  expect_identical(result$end, 6)
  expect_identical(result$text, "Valid line stays")
})

test_that("returns empty data frame when all time cues are malformed", {
  temp_dir_for_test <- withr::local_tempdir()
  srt_all_bad_cues <- c(
    "1",
    "bad --> also bad",
    "Text without valid time"
  )
  temp_srt_file <- helper_create_temp_srt(srt_all_bad_cues, dir = temp_dir_for_test)

  expect_warning(
    result <- import_transcript_from_file(
      normalizePath(temp_srt_file, mustWork = TRUE),
      import_diarization = FALSE
    ),
    "Skipping malformed time cue"
  )
  expect_identical(nrow(result), 0L)
})

test_that("handles time cues but missing subsequent text line correctly", {
  temp_dir_for_test <- withr::local_tempdir()
  srt_missing_text_eof <- c("1", "00:00:01,000 --> 00:00:02,500")
  temp_srt_eof <- helper_create_temp_srt(srt_missing_text_eof, dir = temp_dir_for_test)
  result_srt_eof <- import_transcript_from_file(normalizePath(temp_srt_eof, mustWork = TRUE))
  expect_identical(nrow(result_srt_eof), 1L)
  expect_identical(result_srt_eof$text, NA_character_)

  vtt_missing_text_eof <- c("WEBVTT", "00:00:01.000 --> 00:00:02.500")
  temp_vtt_eof <- helper_create_temp_vtt(vtt_missing_text_eof, dir = temp_dir_for_test)
  result_vtt_eof <- import_transcript_from_file(normalizePath(temp_vtt_eof, mustWork = TRUE))
  expect_identical(nrow(result_vtt_eof), 1L)
  expect_identical(result_vtt_eof$text, NA_character_)

  srt_missing_text_then_cue <- c(
    "1",
    "00:00:01,000 --> 00:00:02,500",
    "2",
    "00:00:03,000 --> 00:00:04,000",
    "Actual text for cue 2"
  )
  temp_srt_mc <- helper_create_temp_srt(srt_missing_text_then_cue, dir = temp_dir_for_test)
  result_srt_mc <- import_transcript_from_file(normalizePath(temp_srt_mc, mustWork = TRUE))
  expect_identical(nrow(result_srt_mc), 2L)
  expect_identical(result_srt_mc$text, c("2", "Actual text for cue 2"))
})

test_that("handles VTT files with various line endings and blank lines", {
  temp_dir_for_test <- withr::local_tempdir()
  vtt_messy <- c(
    "WEBVTT",
    "",
    "00:00:00.000 --> 00:00:01.000",
    "Line one.",
    "",
    "",
    "00:00:02.000 --> 00:00:03.000",
    "Line two.",
    "\\t",
    "00:00:04.000 --> 00:00:05.000",
    "Line three."
  )
  temp_vtt_messy <- helper_create_temp_vtt(vtt_messy, dir = temp_dir_for_test)
  result <- import_transcript_from_file(normalizePath(temp_vtt_messy, mustWork = TRUE))
  expect_identical(nrow(result), 3L)
  expect_identical(result$text, c("Line one.", "Line two.", "Line three."))
})

test_that("handles VTT files with . (dot) as decimal separator for seconds", {
  temp_dir_for_test <- withr::local_tempdir()
  vtt_dot_decimal <- c(
    "WEBVTT",
    "00:00:01.123 --> 00:00:02.456",
    "Text with dot decimal seconds."
  )
  temp_vtt_dot <- helper_create_temp_vtt(vtt_dot_decimal, dir = temp_dir_for_test)
  result <- import_transcript_from_file(normalizePath(temp_vtt_dot, mustWork = TRUE))
  expect_equal(result$start, 1.123)
  expect_equal(result$end, 2.456)
})

test_that("handles SRT files with , (comma) as decimal separator for seconds", {
  temp_dir_for_test <- withr::local_tempdir()
  srt_comma_decimal <- c(
    "1",
    "00:00:01,123 --> 00:00:02,456",
    "Text with comma decimal seconds."
  )
  temp_srt_comma <- helper_create_temp_srt(srt_comma_decimal, dir = temp_dir_for_test)
  result <- import_transcript_from_file(normalizePath(temp_srt_comma, mustWork = TRUE))
  expect_equal(result$start, 1.123)
  expect_equal(result$end, 2.456)
})

test_that("handles multi-line text in SRT correctly", {
  temp_dir_for_test <- withr::local_tempdir()
  srt_multiline_text <- c(
    "1",
    "00:00:01,000 --> 00:00:03,000",
    "This is the first line of text.",
    "This is the second line of text.",
    "",
    "2",
    "00:00:04,000 --> 00:00:05,000",
    "Single line here."
  )
  temp_srt_multi <- helper_create_temp_srt(srt_multiline_text, dir = temp_dir_for_test)
  result <- import_transcript_from_file(normalizePath(temp_srt_multi, mustWork = TRUE))
  expect_identical(nrow(result), 2L)
  expect_identical(result$text, c("This is the first line of text.", "Single line here."))
})

test_that("handles multi-line text in VTT correctly", {
  temp_dir_for_test <- withr::local_tempdir()
  vtt_multiline_text <- c(
    "WEBVTT",
    "00:00:01.000 --> 00:00:03.000",
    "VTT first line.",
    "VTT second line.",
    "",
    "00:00:04.000 --> 00:00:05.000",
    "VTT single line."
  )
  temp_vtt_multi <- helper_create_temp_vtt(vtt_multiline_text, dir = temp_dir_for_test)
  result <- import_transcript_from_file(normalizePath(temp_vtt_multi, mustWork = TRUE))
  expect_identical(nrow(result), 2L)
  expect_identical(result$text, c("VTT first line.", "VTT single line."))
})

# Tests for add_chat_transcript() ----

sample_transcript_data <- data.frame(
  start = c(5, 15, 25),
  end = c(10, 20, 30),
  speaker = c("Alice", "Bob", "Alice"),
  text = c("Hello", "Hi there", "How are you?")
)

parse_chat_webex_file_for_test <- function(chat_file_path, meeting_start_time_str) {
  raw_lines <- readLines(chat_file_path, skipNul = TRUE)
  is_silent_local <- function(x) is.na(x) | stringr::str_trim(x) == ""

  lines_cleaned <- raw_lines |>
    stringr::str_remove_all("\\\\xfe|\\\\xff|\\\\xff\\\\xfe|\\\\xef\\\\xbb\\\\xbf") # BOMs only

  chat_transcript_list <- lines_cleaned |>
    purrr::discard(is_silent_local) |>
    purrr::map(\(x) stringr::str_split_1(x, "[\t]\\s*") |> t())

  if (length(chat_transcript_list) > 0) {
    for (i in rev(seq_along(chat_transcript_list))) {
      if (is.null(chat_transcript_list[[i]])) next
      curr <- chat_transcript_list[[i]]
      if (is.null(curr)) next

      if (ncol(curr) == 1) { # Current line is a continuation line
        if (i > 1) { # Ensure it's not the first line, so prev (i-1) exists
          # Robust check for the previous element before accessing it
          if (!is.null(chat_transcript_list[[i-1]])) {
            prev <- chat_transcript_list[[i - 1]]
            # Ensure prev is a matrix and has columns before trying to access its ncol or elements
            if (is.matrix(prev) && ncol(prev) > 0) {
              target_col_prev <- ncol(prev) # This should be the last column (text)
              chat_transcript_list[[i - 1]][1, target_col_prev] <- paste(
                prev[1, target_col_prev],
                curr[1, 1] # Text from the current (continuation) line
              )
            } # else: prev is not a valid matrix, so we can't merge.
          } # else: prev is NULL, so we can't merge.
          # In any case where merge happens or prev was invalid, mark current line for removal
          chat_transcript_list[[i]] <- NULL
        } else {
          # This is a continuation line and it is the first line (i=1). Discard it.
          chat_transcript_list[[i]] <- NULL
        }
      }
    }
  }
  chat_transcript_list <- purrr::compact(chat_transcript_list)

  if (length(chat_transcript_list) == 0) {
    return(data.frame(start = numeric(0), speaker = character(0), text = character(0)))
  }

  chat_df <- NULL
  tryCatch({
    chat_df_matrix <- do.call("rbind", chat_transcript_list)
    chat_df <- as.data.frame(chat_df_matrix, stringsAsFactors = FALSE)
  }, error = function(e_rbind) {
    return(data.frame(start = numeric(0), speaker = character(0), text = character(0)))
  })

  if (is.null(chat_df)) {
      return(data.frame(start = numeric(0), speaker = character(0), text = character(0)))
  }

  tryCatch({
    chat_df_processed <- chat_df |>
      stats::setNames(c("date_col", "time_str", "speaker_raw", "text_raw")) |>
      dplyr::mutate(
        start = (parse_event_time(time_str) |> as.numeric()) - (parse_event_time(meeting_start_time_str) |> as.numeric()),
        speaker = sub(" from .*", "", speaker_raw) |>
          stringr::str_remove("( to Everyone)?:|( to Me \\\\(Privately\\\\))?:") |>
          stringr::str_replace_all("(?<=\\s)[a-z]", toupper) |>
          stringr::str_squish() |>
          paste0(" (from chat)"),
        text = stringr::str_squish(text_raw)
      ) |>
      dplyr::select(start, speaker, text)
    return(chat_df_processed)
  }, error = function(e_processed) {
    return(data.frame(start = numeric(0), speaker = character(0), text = character(0)))
  })
}

test_that("adds chat from a WebEx file correctly (testing parsing logic)", {
  temp_dir_for_test <- withr::local_tempdir()
  chat_content_lines <- c(
    "02/20/2023\t00:00:02\tUser One from User One to Everyone:\tFirst message",
    "02/20/2023\t00:00:12\tUser Two from User Two to Me (Privately):\tSecond message",
    "02/20/2023\t00:00:22\tUser Three from User Three to Everyone:\tThis is a message",
    "that spans two lines in the raw file."
  )
  temp_chat_file <- helper_create_temp_chat_file(chat_content_lines, dir = temp_dir_for_test)
  meeting_start_hms <- "00:00:00"

  parsed_chat_df <- parse_chat_webex_file_for_test(normalizePath(temp_chat_file, mustWork = TRUE), meeting_start_hms) |> dplyr::as_tibble()

  expected_df <- dplyr::tibble(
    start = c(2, 12, 22),
    speaker = c("User One (from chat)", "User Two (from chat)", "User Three (from chat)"),
    text = c("First message", "Second message", "This is a message that spans two lines in the raw file.")
  )
  expect_equal(parsed_chat_df, expected_df)
})

test_that("add_chat_transcript interleaves chat rows with transcript data", {
  temp_dir_for_test <- withr::local_tempdir()
  chat_content_lines <- c(
    "02/20/2023\t00:00:02\tUser One from User One to Everyone:\tFirst message",
    "02/20/2023\t00:00:12\tUser Two from User Two to Me (Privately):\tSecond message",
    "02/20/2023\t00:00:22\tUser Three from User Three to Everyone:\tThis is a message",
    "that spans two lines in the raw file."
  )
  temp_chat_file <- helper_create_temp_chat_file(chat_content_lines, dir = temp_dir_for_test)

  result <- add_chat_transcript(
    transcript_data = sample_transcript_data,
    chat_transcript = normalizePath(temp_chat_file, mustWork = TRUE),
    start_time = "00:00:00"
  )

  expected <- dplyr::tibble(
    start = c(2, 5, 12, 15, 22, 25),
    end = c(2, 10, 12, 20, 22, 30),
    speaker = c(
      "User One (from chat)",
      "Alice",
      "User Two (from chat)",
      "Bob",
      "User Three (from chat)",
      "Alice"
    ),
    text = c(
      "First message",
      "Hello",
      "Second message",
      "Hi there",
      "This is a message that spans two lines in the raw file.",
      "How are you?"
    )
  )

  expect_equal(result, expected)
})

test_that("add_chat_transcript handles pre-parsed data.frame correctly", {
  chat_df <- data.frame(
    start = c(8, 18),
    speaker = c("Chatter Alpha", "Chatter Bravo"),
    text = c("Chat message one", "Chat message two"),
    stringsAsFactors = FALSE
  )
  result <- add_chat_transcript(
    transcript_data = sample_transcript_data,
    chat_transcript = chat_df,
    start_time = "00:00:00"
  )
  expected <- dplyr::tibble(
    start = c(5, 8, 15, 18, 25),
    end = c(10, 8, 20, 18, 30),
    speaker = c("Alice", "Chatter Alpha", "Bob", "Chatter Bravo", "Alice"),
    text = c("Hello", "Chat message one", "Hi there", "Chat message two", "How are you?")
  )
  expect_equal(result, expected)

  bad_chat_df <- data.frame(s = 1, p = "P", t = "T")
  expect_error(
    add_chat_transcript(sample_transcript_data, bad_chat_df, "00:00:00"),
    regexp = "Chat data must contain `start`, `speaker` and `text` columns\\."
  )
})

test_that("add_chat_transcript start_time argument impacts chat file parsing", {
  temp_dir_for_test <- withr::local_tempdir()
  abs_chat_lines <- c("01/01/2024\t10:00:30\tTester Bot\tMsg1")
  temp_abs_chat_file <- abs_chat_lines |>
    helper_create_temp_chat_file(dir = temp_dir_for_test) |>
    normalizePath(mustWork = TRUE)

  parsed_chat_data_1000 <- temp_abs_chat_file |>
    parse_chat_webex_file_for_test("10:00:00") |>
    dplyr::as_tibble()

  expected_1000 <- dplyr::tibble(
    start = 30,
    speaker = "Tester Bot (from chat)",
    text = "Msg1"
  )
  expect_equal(parsed_chat_data_1000, expected_1000)

  parsed_chat_data_0959 <- temp_abs_chat_file |>
    parse_chat_webex_file_for_test("09:59:50") |>
    dplyr::as_tibble()

  expected_0959 <- dplyr::tibble(
    start = 40,
    speaker = "Tester Bot (from chat)",
    text = "Msg1"
  )
  expect_equal(parsed_chat_data_0959, expected_0959)

  parsed_chat_data_9am <- temp_abs_chat_file |>
    parse_chat_webex_file_for_test("9:00 AM") |>
    dplyr::as_tibble()

  expected_9am <- dplyr::tibble(
    # 10:00:30 AM is 36030 seconds from midnight. 9:00 AM is 32400 seconds.
    # Diff = 3630
    start = 3630,
    speaker = "Tester Bot (from chat)",
    text = "Msg1"
  )
  expect_equal(parsed_chat_data_9am, expected_9am)
})

test_that("error handling in add_chat_transcript for file/format issues", {
  temp_dir_for_test <- withr::local_tempdir()
  expect_error(
    add_chat_transcript(sample_transcript_data, "nonexistent_chat_file.txt", "10:00"),
    "Chat file not found."
  )

  temp_bad_chat_file <- file.path(temp_dir_for_test, "bad_chat.txt")
  writeLines("This is not a valid webex chat format line", temp_bad_chat_file)
  expect_warning(
    expect_error(
      add_chat_transcript(
        sample_transcript_data,
        normalizePath(temp_bad_chat_file, mustWork = TRUE),
        "10:00"
      ),
      "Error parsing chat file"
    ),
    "Skipping chat line that looks like a continuation but has no previous message."
  )

  expect_error(
    add_chat_transcript(sample_transcript_data, 12345, "10:00"),
    "Unsupported chat data format."
  )

  expect_error(
    add_chat_transcript(sample_transcript_data, normalizePath(temp_bad_chat_file, mustWork = TRUE), "10:00", chat_format = "teams"),
    "should be \"webex\""
  )
})

test_that("chat parsing handles empty chat files or files with only silent lines", {
  temp_dir_for_test <- withr::local_tempdir()
  temp_empty_chat <- character(0) |>
    helper_create_temp_chat_file(dir = temp_dir_for_test) |>
    normalizePath(mustWork = TRUE)

  parsed_empty <- parse_chat_webex_file_for_test(temp_empty_chat, "00:00:00")

  expect_s3_class(parsed_empty, "data.frame")
  expect_identical(nrow(parsed_empty), 0L)
  expect_identical(names(parsed_empty), c("start", "speaker", "text"))

  temp_silent_chat <- c("", "   ") |>
    helper_create_temp_chat_file(dir = temp_dir_for_test) |>
    normalizePath(mustWork = TRUE)

  parsed_silent <- parse_chat_webex_file_for_test(temp_silent_chat, "00:00:00")

  expect_s3_class(parsed_silent, "data.frame")
  expect_identical(nrow(parsed_silent), 0L)
})

# Final newline
